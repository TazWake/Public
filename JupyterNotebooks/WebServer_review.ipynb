{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Server Log Analysis\n",
    "\n",
    "This notebook analyzes web server logs in Common or Combined Log Format.\n",
    "\n",
    "## Features\n",
    "1. Parse Apache/Nginx logs (Common and Combined formats)\n",
    "2. Identify top 10 most frequent IP addresses with visualization\n",
    "3. Statistical analysis of HTTP request methods\n",
    "4. Detection of invalid/junk HTTP methods with source IPs\n",
    "5. Top 10 User Agent strings analysis (if available)\n",
    "\n",
    "## Log Formats Supported\n",
    "\n",
    "**Common Log Format:**\n",
    "```\n",
    "127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326\n",
    "```\n",
    "\n",
    "**Combined Log Format:**\n",
    "```\n",
    "127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \"GET /apache_pb.gif HTTP/1.0\" 200 2326 \"http://www.example.com/start.html\" \"Mozilla/4.08 [en] (Win98; I ;Nav)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set the path to your web server log file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "LOG_FILE_PATH = '/path/to/your/access.log'  # Update this path\n",
    "\n",
    "# Valid HTTP methods (RFC 7231, RFC 5789)\n",
    "VALID_HTTP_METHODS = [\n",
    "    'GET', 'POST', 'PUT', 'DELETE', 'HEAD', \n",
    "    'OPTIONS', 'PATCH', 'CONNECT', 'TRACE'\n",
    "]\n",
    "\n",
    "print(f\"Configuration set. Will analyze: {LOG_FILE_PATH}\")\n",
    "print(f\"Valid HTTP methods: {', '.join(VALID_HTTP_METHODS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Parse Web Server Logs\n",
    "\n",
    "Parse logs using regex patterns for both Common and Combined formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_line(line):\n",
    "    \"\"\"\n",
    "    Parse a single log line in Common or Combined format.\n",
    "    \n",
    "    Returns a dictionary with parsed fields or None if parsing fails.\n",
    "    \"\"\"\n",
    "    # Combined log format regex pattern\n",
    "    # Matches: IP - user [timestamp] \"METHOD /path HTTP/version\" status size \"referrer\" \"user-agent\"\n",
    "    combined_pattern = re.compile(\n",
    "        r'^(?P<ip>[\\d\\.]+) '\n",
    "        r'- '\n",
    "        r'(?P<user>\\S+) '\n",
    "        r'\\[(?P<timestamp>[^\\]]+)\\] '\n",
    "        r'\"(?P<method>\\S+) '\n",
    "        r'(?P<path>\\S+) '\n",
    "        r'(?P<protocol>\\S+)\" '\n",
    "        r'(?P<status>\\d+) '\n",
    "        r'(?P<size>\\S+)'\n",
    "        r'(?: \"(?P<referrer>[^\"]*)\" '\n",
    "        r'\"(?P<user_agent>[^\"]*)\")?'\n",
    "    )\n",
    "    \n",
    "    # Try combined format first\n",
    "    match = combined_pattern.match(line)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    \n",
    "    # Try common format (without referrer and user agent)\n",
    "    common_pattern = re.compile(\n",
    "        r'^(?P<ip>[\\d\\.]+) '\n",
    "        r'- '\n",
    "        r'(?P<user>\\S+) '\n",
    "        r'\\[(?P<timestamp>[^\\]]+)\\] '\n",
    "        r'\"(?P<method>\\S+) '\n",
    "        r'(?P<path>\\S+) '\n",
    "        r'(?P<protocol>\\S+)\" '\n",
    "        r'(?P<status>\\d+) '\n",
    "        r'(?P<size>\\S+)'\n",
    "    )\n",
    "    \n",
    "    match = common_pattern.match(line)\n",
    "    if match:\n",
    "        data = match.groupdict()\n",
    "        data['referrer'] = None\n",
    "        data['user_agent'] = None\n",
    "        return data\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_and_parse_logs(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse all log entries from the specified file.\n",
    "    \n",
    "    Returns a pandas DataFrame with parsed log entries.\n",
    "    \"\"\"\n",
    "    parsed_logs = []\n",
    "    failed_lines = 0\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            for line_num, line in enumerate(f, 1):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                    \n",
    "                parsed = parse_log_line(line)\n",
    "                if parsed:\n",
    "                    parsed_logs.append(parsed)\n",
    "                else:\n",
    "                    failed_lines += 1\n",
    "                    if failed_lines <= 5:  # Show first 5 failed lines\n",
    "                        print(f\"Warning: Failed to parse line {line_num}: {line[:100]}...\")\n",
    "        \n",
    "        df = pd.DataFrame(parsed_logs)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Successfully parsed {len(parsed_logs)} log entries\")\n",
    "        print(f\"Failed to parse {failed_lines} lines\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Load and parse the logs\n",
    "df_logs = load_and_parse_logs(LOG_FILE_PATH)\n",
    "\n",
    "if not df_logs.empty:\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    print(df_logs.info())\n",
    "    print(\"\\nFirst 5 entries:\")\n",
    "    display(df_logs.head())\n",
    "else:\n",
    "    print(\"No data loaded. Please check the LOG_FILE_PATH and ensure the file exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Top 10 Most Frequent IP Addresses\n",
    "\n",
    "Analyze and visualize the most active IP addresses accessing the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_logs.empty:\n",
    "    # Count IP address frequencies\n",
    "    ip_counts = df_logs['ip'].value_counts().head(10)\n",
    "    \n",
    "    print(\"Top 10 Most Frequent IP Addresses:\")\n",
    "    print(\"=\"*60)\n",
    "    for idx, (ip, count) in enumerate(ip_counts.items(), 1):\n",
    "        percentage = (count / len(df_logs)) * 100\n",
    "        print(f\"{idx:2d}. {ip:15s} - {count:6d} requests ({percentage:5.2f}%)\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar chart\n",
    "    colors = sns.color_palette('viridis', len(ip_counts))\n",
    "    ip_counts.plot(kind='barh', ax=ax1, color=colors)\n",
    "    ax1.set_xlabel('Number of Requests', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('IP Address', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('Top 10 IP Addresses by Request Count', fontsize=14, fontweight='bold')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(ip_counts.values):\n",
    "        ax1.text(v + (max(ip_counts.values) * 0.01), i, str(v), \n",
    "                va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Pie chart\n",
    "    ax2.pie(ip_counts.values, labels=ip_counts.index, autopct='%1.1f%%',\n",
    "            startangle=90, colors=colors)\n",
    "    ax2.set_title('Top 10 IP Addresses - Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Additional statistics\n",
    "    total_unique_ips = df_logs['ip'].nunique()\n",
    "    print(f\"\\nTotal unique IP addresses: {total_unique_ips}\")\n",
    "    print(f\"Top 10 IPs account for {(ip_counts.sum() / len(df_logs) * 100):.2f}% of all requests\")\n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: HTTP Request Method Analysis\n",
    "\n",
    "Statistical breakdown of HTTP methods used in requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_logs.empty:\n",
    "    # Count HTTP methods\n",
    "    method_counts = df_logs['method'].value_counts()\n",
    "    \n",
    "    print(\"HTTP Request Method Statistics:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Method':<15} {'Count':>10} {'Percentage':>12} {'Valid':>10}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for method, count in method_counts.items():\n",
    "        percentage = (count / len(df_logs)) * 100\n",
    "        is_valid = 'Yes' if method.upper() in VALID_HTTP_METHODS else 'No'\n",
    "        print(f\"{method:<15} {count:>10} {percentage:>11.2f}% {is_valid:>10}\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Total Requests':<15} {len(df_logs):>10}\")\n",
    "    print(f\"{'Unique Methods':<15} {len(method_counts):>10}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Bar chart for all methods\n",
    "    colors = ['green' if m.upper() in VALID_HTTP_METHODS else 'red' \n",
    "              for m in method_counts.index]\n",
    "    method_counts.plot(kind='bar', ax=ax1, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    ax1.set_xlabel('HTTP Method', fontsize=12, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Requests', fontsize=12, fontweight='bold')\n",
    "    ax1.set_title('HTTP Method Distribution (Green=Valid, Red=Invalid)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(method_counts.values):\n",
    "        ax1.text(i, v + (max(method_counts.values) * 0.01), str(v),\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Pie chart for valid vs invalid\n",
    "    valid_count = df_logs[df_logs['method'].str.upper().isin(VALID_HTTP_METHODS)].shape[0]\n",
    "    invalid_count = len(df_logs) - valid_count\n",
    "    \n",
    "    ax2.pie([valid_count, invalid_count], \n",
    "            labels=['Valid Methods', 'Invalid/Junk Methods'],\n",
    "            autopct='%1.1f%%',\n",
    "            colors=['green', 'red'],\n",
    "            startangle=90,\n",
    "            explode=(0, 0.1))\n",
    "    ax2.set_title('Valid vs Invalid HTTP Methods', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical summary\n",
    "    print(f\"\\nStatistical Summary:\")\n",
    "    print(f\"Valid HTTP method requests: {valid_count} ({(valid_count/len(df_logs)*100):.2f}%)\")\n",
    "    print(f\"Invalid HTTP method requests: {invalid_count} ({(invalid_count/len(df_logs)*100):.2f}%)\")\n",
    "    \n",
    "    # Most common valid method\n",
    "    valid_methods = df_logs[df_logs['method'].str.upper().isin(VALID_HTTP_METHODS)]\n",
    "    if not valid_methods.empty:\n",
    "        most_common = valid_methods['method'].value_counts().iloc[0]\n",
    "        most_common_name = valid_methods['method'].value_counts().index[0]\n",
    "        print(f\"Most common valid method: {most_common_name} ({most_common} requests)\")\n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Identify Junk/Invalid HTTP Methods\n",
    "\n",
    "Detect non-standard or malicious HTTP methods and their source IPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_logs.empty:\n",
    "    # Filter for invalid methods\n",
    "    invalid_methods = df_logs[~df_logs['method'].str.upper().isin(VALID_HTTP_METHODS)]\n",
    "    \n",
    "    if not invalid_methods.empty:\n",
    "        print(f\"Found {len(invalid_methods)} requests with invalid/junk HTTP methods\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Group by method and IP\n",
    "        junk_analysis = invalid_methods.groupby(['method', 'ip']).size().reset_index(name='count')\n",
    "        junk_analysis = junk_analysis.sort_values('count', ascending=False)\n",
    "        \n",
    "        print(f\"\\n{'Method':<20} {'IP Address':<20} {'Count':>10}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for _, row in junk_analysis.iterrows():\n",
    "            print(f\"{row['method']:<20} {row['ip']:<20} {row['count']:>10}\")\n",
    "        \n",
    "        # Summary by method\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"\\nInvalid Methods Summary:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        method_summary = invalid_methods.groupby('method').agg({\n",
    "            'ip': ['count', 'nunique']\n",
    "        }).round(2)\n",
    "        method_summary.columns = ['Total Requests', 'Unique IPs']\n",
    "        method_summary = method_summary.sort_values('Total Requests', ascending=False)\n",
    "        \n",
    "        print(method_summary)\n",
    "        \n",
    "        # Show sample invalid requests\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Sample Invalid Requests (first 10):\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        sample_columns = ['ip', 'method', 'path', 'status', 'timestamp']\n",
    "        display(invalid_methods[sample_columns].head(10))\n",
    "        \n",
    "        # Visualization\n",
    "        if len(junk_analysis) > 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "            \n",
    "            # Top invalid methods\n",
    "            top_junk_methods = invalid_methods['method'].value_counts().head(10)\n",
    "            top_junk_methods.plot(kind='barh', ax=ax1, color='crimson', edgecolor='black')\n",
    "            ax1.set_xlabel('Number of Requests', fontsize=12, fontweight='bold')\n",
    "            ax1.set_ylabel('Invalid Method', fontsize=12, fontweight='bold')\n",
    "            ax1.set_title('Top 10 Invalid/Junk HTTP Methods', fontsize=14, fontweight='bold')\n",
    "            ax1.invert_yaxis()\n",
    "            \n",
    "            # Top IPs sending invalid methods\n",
    "            top_junk_ips = invalid_methods['ip'].value_counts().head(10)\n",
    "            top_junk_ips.plot(kind='barh', ax=ax2, color='orange', edgecolor='black')\n",
    "            ax2.set_xlabel('Number of Invalid Requests', fontsize=12, fontweight='bold')\n",
    "            ax2.set_ylabel('IP Address', fontsize=12, fontweight='bold')\n",
    "            ax2.set_title('Top 10 IPs Sending Invalid Methods', fontsize=14, fontweight='bold')\n",
    "            ax2.invert_yaxis()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Export to CSV for further investigation\n",
    "        output_file = 'invalid_http_methods.csv'\n",
    "        junk_analysis.to_csv(output_file, index=False)\n",
    "        print(f\"\\nInvalid methods data exported to: {output_file}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No invalid/junk HTTP methods found in the logs.\")\n",
    "        print(\"All requests use standard HTTP methods.\")\n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: User Agent Analysis\n",
    "\n",
    "Analyze User Agent strings to identify common browsers, bots, and suspicious agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_logs.empty:\n",
    "    # Check if user agent data is available\n",
    "    has_user_agent = 'user_agent' in df_logs.columns and df_logs['user_agent'].notna().any()\n",
    "    \n",
    "    if has_user_agent:\n",
    "        # Filter out null/empty user agents\n",
    "        ua_data = df_logs[df_logs['user_agent'].notna() & (df_logs['user_agent'] != '')]\n",
    "        \n",
    "        if len(ua_data) > 0:\n",
    "            print(f\"Found {len(ua_data)} requests with User Agent strings\")\n",
    "            print(f\"Requests without User Agent: {len(df_logs) - len(ua_data)}\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Top 10 User Agents\n",
    "            top_user_agents = ua_data['user_agent'].value_counts().head(10)\n",
    "            \n",
    "            print(\"\\nTop 10 Most Common User Agent Strings:\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            for idx, (ua, count) in enumerate(top_user_agents.items(), 1):\n",
    "                percentage = (count / len(ua_data)) * 100\n",
    "                # Truncate long UA strings for display\n",
    "                ua_display = ua[:70] + '...' if len(ua) > 70 else ua\n",
    "                print(f\"\\n{idx}. {ua_display}\")\n",
    "                print(f\"   Count: {count:,} ({percentage:.2f}%)\")\n",
    "            \n",
    "            # Categorize User Agents\n",
    "            def categorize_user_agent(ua_string):\n",
    "                \"\"\"Categorize user agent into browser, bot, or other.\"\"\"\n",
    "                if pd.isna(ua_string) or ua_string == '':\n",
    "                    return 'Unknown'\n",
    "                \n",
    "                ua_lower = ua_string.lower()\n",
    "                \n",
    "                # Bots and crawlers\n",
    "                bot_keywords = ['bot', 'crawler', 'spider', 'scraper', 'curl', 'wget', \n",
    "                               'python', 'java', 'perl', 'ruby', 'scan']\n",
    "                if any(keyword in ua_lower for keyword in bot_keywords):\n",
    "                    return 'Bot/Crawler'\n",
    "                \n",
    "                # Browsers\n",
    "                if 'chrome' in ua_lower or 'chromium' in ua_lower:\n",
    "                    return 'Chrome'\n",
    "                elif 'firefox' in ua_lower:\n",
    "                    return 'Firefox'\n",
    "                elif 'safari' in ua_lower and 'chrome' not in ua_lower:\n",
    "                    return 'Safari'\n",
    "                elif 'edge' in ua_lower or 'edg/' in ua_lower:\n",
    "                    return 'Edge'\n",
    "                elif 'msie' in ua_lower or 'trident' in ua_lower:\n",
    "                    return 'Internet Explorer'\n",
    "                elif 'opera' in ua_lower or 'opr/' in ua_lower:\n",
    "                    return 'Opera'\n",
    "                \n",
    "                return 'Other'\n",
    "            \n",
    "            # Apply categorization\n",
    "            ua_data['ua_category'] = ua_data['user_agent'].apply(categorize_user_agent)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"User Agent Categories:\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            category_counts = ua_data['ua_category'].value_counts()\n",
    "            for category, count in category_counts.items():\n",
    "                percentage = (count / len(ua_data)) * 100\n",
    "                print(f\"{category:<20} {count:>10,} ({percentage:>6.2f}%)\")\n",
    "            \n",
    "            # Visualizations\n",
    "            fig = plt.figure(figsize=(16, 10))\n",
    "            gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
    "            \n",
    "            # Top 10 User Agents bar chart\n",
    "            ax1 = fig.add_subplot(gs[0, :])\n",
    "            top_10_for_plot = top_user_agents.head(10)\n",
    "            # Truncate labels for visualization\n",
    "            labels = [ua[:50] + '...' if len(ua) > 50 else ua for ua in top_10_for_plot.index]\n",
    "            ax1.barh(range(len(top_10_for_plot)), top_10_for_plot.values, color='teal', edgecolor='black')\n",
    "            ax1.set_yticks(range(len(top_10_for_plot)))\n",
    "            ax1.set_yticklabels(labels, fontsize=9)\n",
    "            ax1.set_xlabel('Number of Requests', fontsize=12, fontweight='bold')\n",
    "            ax1.set_title('Top 10 User Agent Strings', fontsize=14, fontweight='bold')\n",
    "            ax1.invert_yaxis()\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, v in enumerate(top_10_for_plot.values):\n",
    "                ax1.text(v + (max(top_10_for_plot.values) * 0.01), i, f'{v:,}',\n",
    "                        va='center', fontsize=10, fontweight='bold')\n",
    "            \n",
    "            # Category pie chart\n",
    "            ax2 = fig.add_subplot(gs[1, 0])\n",
    "            colors_cat = sns.color_palette('Set2', len(category_counts))\n",
    "            ax2.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "                   startangle=90, colors=colors_cat)\n",
    "            ax2.set_title('User Agent Categories', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            # Requests with/without UA\n",
    "            ax3 = fig.add_subplot(gs[1, 1])\n",
    "            ua_presence = [\n",
    "                len(df_logs[df_logs['user_agent'].notna() & (df_logs['user_agent'] != '')]),\n",
    "                len(df_logs[df_logs['user_agent'].isna() | (df_logs['user_agent'] == '')])\n",
    "            ]\n",
    "            ax3.pie(ua_presence, labels=['With User Agent', 'Without User Agent'],\n",
    "                   autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'lightcoral'])\n",
    "            ax3.set_title('User Agent Presence', fontsize=14, fontweight='bold')\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "            # Detect suspicious User Agents\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Potentially Suspicious User Agents:\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            suspicious_keywords = ['scan', 'exploit', 'attack', 'hack', 'inject', 'sqlmap', \n",
    "                                  'nikto', 'nmap', 'masscan', 'nessus', 'metasploit']\n",
    "            \n",
    "            suspicious_ua = ua_data[ua_data['user_agent'].str.lower().str.contains(\n",
    "                '|'.join(suspicious_keywords), na=False, regex=True\n",
    "            )]\n",
    "            \n",
    "            if len(suspicious_ua) > 0:\n",
    "                print(f\"Found {len(suspicious_ua)} requests with suspicious User Agents:\\n\")\n",
    "                \n",
    "                susp_summary = suspicious_ua.groupby(['user_agent', 'ip']).size().reset_index(name='count')\n",
    "                susp_summary = susp_summary.sort_values('count', ascending=False)\n",
    "                \n",
    "                for _, row in susp_summary.head(20).iterrows():\n",
    "                    ua_display = row['user_agent'][:70] + '...' if len(row['user_agent']) > 70 else row['user_agent']\n",
    "                    print(f\"UA: {ua_display}\")\n",
    "                    print(f\"IP: {row['ip']}, Count: {row['count']}\\n\")\n",
    "                \n",
    "                # Export suspicious UAs\n",
    "                susp_output = 'suspicious_user_agents.csv'\n",
    "                susp_summary.to_csv(susp_output, index=False)\n",
    "                print(f\"Suspicious User Agents exported to: {susp_output}\")\n",
    "            else:\n",
    "                print(\"No obviously suspicious User Agent strings detected.\")\n",
    "            \n",
    "            # Export top User Agents\n",
    "            ua_output = 'top_user_agents.csv'\n",
    "            top_user_agents.to_csv(ua_output, header=['Count'])\n",
    "            print(f\"\\nTop User Agents exported to: {ua_output}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"User Agent field exists but contains no data.\")\n",
    "    else:\n",
    "        print(\"User Agent information not available in this log format.\")\n",
    "        print(\"The logs appear to be in Common Log Format (without User Agent field).\")\n",
    "        print(\"For User Agent analysis, use Combined Log Format.\")\n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report\n",
    "\n",
    "Generate a comprehensive summary of the log analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_logs.empty:\n",
    "    print(\"=\"*80)\n",
    "    print(\" \" * 25 + \"WEB SERVER LOG ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nLog File: {LOG_FILE_PATH}\")\n",
    "    print(f\"Total Requests Analyzed: {len(df_logs):,}\")\n",
    "    print(f\"Date Range: {df_logs['timestamp'].min()} to {df_logs['timestamp'].max()}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"IP ADDRESS STATISTICS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Total Unique IP Addresses: {df_logs['ip'].nunique():,}\")\n",
    "    print(f\"Most Active IP: {df_logs['ip'].value_counts().index[0]} ({df_logs['ip'].value_counts().iloc[0]:,} requests)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"HTTP METHOD STATISTICS\")\n",
    "    print(\"-\"*80)\n",
    "    method_stats = df_logs['method'].value_counts()\n",
    "    print(f\"Total Unique Methods: {len(method_stats)}\")\n",
    "    print(f\"Most Common Method: {method_stats.index[0]} ({method_stats.iloc[0]:,} requests)\")\n",
    "    \n",
    "    valid_count = df_logs[df_logs['method'].str.upper().isin(VALID_HTTP_METHODS)].shape[0]\n",
    "    invalid_count = len(df_logs) - valid_count\n",
    "    print(f\"Valid HTTP Methods: {valid_count:,} ({(valid_count/len(df_logs)*100):.2f}%)\")\n",
    "    print(f\"Invalid HTTP Methods: {invalid_count:,} ({(invalid_count/len(df_logs)*100):.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"HTTP STATUS CODE STATISTICS\")\n",
    "    print(\"-\"*80)\n",
    "    status_stats = df_logs['status'].value_counts().head(5)\n",
    "    for status, count in status_stats.items():\n",
    "        percentage = (count / len(df_logs)) * 100\n",
    "        print(f\"Status {status}: {count:,} ({percentage:.2f}%)\")\n",
    "    \n",
    "    if 'user_agent' in df_logs.columns:\n",
    "        ua_present = df_logs['user_agent'].notna().sum()\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"USER AGENT STATISTICS\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"Requests with User Agent: {ua_present:,} ({(ua_present/len(df_logs)*100):.2f}%)\")\n",
    "        print(f\"Requests without User Agent: {len(df_logs) - ua_present:,}\")\n",
    "        if ua_present > 0:\n",
    "            print(f\"Unique User Agents: {df_logs['user_agent'].nunique():,}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Analysis completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # List exported files\n",
    "    print(\"\\nExported Files:\")\n",
    "    import os\n",
    "    exported_files = ['invalid_http_methods.csv', 'suspicious_user_agents.csv', 'top_user_agents.csv']\n",
    "    for file in exported_files:\n",
    "        if os.path.exists(file):\n",
    "            print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"No analysis performed. Please check the log file path and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Analysis (Optional)\n",
    "\n",
    "Perform additional custom analysis as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze status codes by IP\n",
    "if not df_logs.empty:\n",
    "    print(\"Status Code Distribution by Top 5 IPs:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    top_5_ips = df_logs['ip'].value_counts().head(5).index\n",
    "    \n",
    "    for ip in top_5_ips:\n",
    "        ip_data = df_logs[df_logs['ip'] == ip]\n",
    "        status_dist = ip_data['status'].value_counts()\n",
    "        print(f\"\\nIP: {ip}\")\n",
    "        for status, count in status_dist.items():\n",
    "            print(f\"  Status {status}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find most requested paths\n",
    "if not df_logs.empty:\n",
    "    print(\"\\nTop 10 Most Requested Paths:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    top_paths = df_logs['path'].value_counts().head(10)\n",
    "    for path, count in top_paths.items():\n",
    "        print(f\"{path:<50} {count:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Forensic Considerations\n",
    "- Invalid HTTP methods may indicate:\n",
    "  - Reconnaissance/scanning activity\n",
    "  - Misconfigured clients or bots\n",
    "  - Exploitation attempts\n",
    "  - Web application attacks\n",
    "\n",
    "### Valid HTTP Methods (RFC 7231, RFC 5789)\n",
    "- **GET**: Retrieve resource\n",
    "- **POST**: Submit data to resource\n",
    "- **PUT**: Replace resource\n",
    "- **DELETE**: Remove resource\n",
    "- **HEAD**: Retrieve headers only\n",
    "- **OPTIONS**: Describe communication options\n",
    "- **PATCH**: Partial resource modification\n",
    "- **CONNECT**: Establish tunnel (proxy)\n",
    "- **TRACE**: Echo request (debugging)\n",
    "\n",
    "### Common Attack Signatures\n",
    "- Suspicious methods: `PROPFIND`, `SEARCH`, `LOCK`, `UNLOCK` (WebDAV)\n",
    "- Scanning tools: `sqlmap`, `nikto`, `nmap`, `masscan` in User Agent\n",
    "- Abnormal request patterns from single IP\n",
    "- Requests for known vulnerable paths (e.g., `/admin`, `/wp-admin`, `/.env`)\n",
    "\n",
    "### Next Steps\n",
    "1. Investigate IPs with high invalid method counts\n",
    "2. Cross-reference suspicious IPs with threat intelligence feeds\n",
    "3. Review paths requested by suspicious IPs\n",
    "4. Check for temporal patterns (time-based attacks)\n",
    "5. Correlate with other security logs (firewall, IDS/IPS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
